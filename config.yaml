# --- Dataset Configuration ---
data_dir: '/kaggle/input/malaria-full-class/v2_malaria_full_class_classification' # Base dir for relative paths if needed

# --- Datasets Configuration (List) ---
datasets:
  - type: 'annotation' # Optional: Second dataset definition (e.g., original training data)
    annotation_train: '/kaggle/input/malaria-full-class/v2_malaria_full_class_classification/train_annotation_6classes.txt'
    annotation_val: '/kaggle/input/malaria-full-class/v2_malaria_full_class_classification/val_annotation_6classes.txt' # Use original val
    annotation_test: '/kaggle/input/malaria-full-class/v2_malaria_full_class_classification/test_annotation_6classes.txt' # Use original test
    annotation_root: '/kaggle/input/malaria-full-class/v2_malaria_full_class_classification'

  # - type: 'annotation' # First dataset definition (e.g., SMOTE for training)
  #   # Settings for 'annotation' type
  #   annotation_train: '/kaggle/input/datn-smote/smote_custom_strategy_output_structured/smote_annotations.txt'
  #   annotation_val: null # Use original val
  #   annotation_test: null # Use original test
  #   annotation_root: '/kaggle/input/datn-smote/smote_custom_strategy_output_structured' # Root for THIS dataset's images
  #   # If val/test annotations use different root, specify explicitly or adjust AnnotationDataset
  #   # annotation_val_root: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/cropped_RBCs'
  #   # annotation_test_root: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/cropped_RBCs'


  # - type: 'imagefolder' # Optional: Example ImageFolder definition
  #   imagefolder_root: '/kaggle/input/bbbc041/dataset_resplit'
  #   imagefolder_train_subdir: 'train'
  #   imagefolder_val_subdir: 'val'
  #   imagefolder_test_subdir: 'test'

batch_size: 32
num_workers: 4 # Adjusted based on typical Kaggle limits

# # --- Class Configuration ---
# class_names:
#   - "Ring"
#   - "Trophozoite"
#   - "Schizont"
#   - "Gametocyte" # Corrected spelling
#   - "Healthy RBC"
#   - "Other"
# --- Class Configuration ---
class_names:
  - "rbc_parasitized_F_TJ"
  - "rbc_parasitized_F_TA"
  - "rbc_parasitized_F_S1_or_S2"
  - "rbc_parasitized_F_G1_or_G2"
  - "rbc_unparasitized"
  - "rbc_unparasitized_artefact_dead_kernel"

# --- Class Remapping Configuration (NEW) ---
class_remapping:
  enabled: true                    # Set to true to enable class remapping
  mapping:                         # Dictionary mapping: original_class_index -> new_class_index
    5: 4                          # Map class 5 to class 4 (merge artefact into unparasitized)
  # Example of more complex remapping:
  # mapping:
  #   1: 0    # Map class 1 to class 0
  #   2: 0    # Map class 2 to class 0  
  #   3: 1    # Map class 3 to class 1
  #   4: 2    # Map class 4 to class 2
  final_class_names:              # New class names after remapping (optional)
    - "rbc_parasitized_F_TJ"
    - "rbc_parasitized_F_TA" 
    - "rbc_parasitized_F_S1_or_S2"
    - "rbc_parasitized_F_G1_or_G2"
    - "rbc_unparasitized_combined"  # Combined class

# --- Model Configuration ---
model_names:
  # - mobilenetv4_hybrid_large.e600_r384_in1k # Example timm model
  # - resnet50 # Example torchvision model
  - mobilenetv4_hybrid_medium.ix_e550_r384_in1k

# --- Training Configuration ---
training:
  dropout_rate: 0.2 # Dropout rate for the model
  num_epochs: 100 # Reduced for example
  patience: 20   # Early stopping patience
  use_amp: false # Use Automatic Mixed Precision (requires CUDA)
  clip_grad_norm: 0 # Max norm for gradient clipping (set to 0 or null to disable)
  train_ratio: 1 # Ratio of the (combined) training set to use (1.0 = 100%, 0.1 = 10%)
  first_stage_epochs: 0 # Number of epochs to use criterion_a before switching to criterion_b

# --- Optimizer Configuration ---
optimizer:
  type: Adam
  lr: 0.0001    # Typical default for Adam, adjust as needed
  params:
    weight_decay: 0.0005 # Typical value for Adam, adjust as needed

# --- LR Scheduler Configuration ---
scheduler:
  type: ReduceLROnPlateau
  mode: 'max'         # Use 'max' if monitoring accuracy, 'min' for loss
  factor: 0.5         # Reduce LR by a factor of 0.5
  patience: 5         # Number of epochs with no improvement before reducing LR
  threshold: 0.01     # Minimum change to qualify as improvement
  min_lr: 0.000001        # Lower bound on the learning rate


# --- Loss Function (Criterion) Configuration ---
criterion_a: crossentropyloss  # First criterion to use
criterion_a_params:     # Parameters for first criterion
  # use_class_weights: false
  
criterion_b: crossentropyloss    # Second criterion to use (after first_stage_epochs)
criterion_b_params:     # Parameters for second criterion
  # use_class_weights: false
  # gamma: 5.0
  # alpha: 1

# legacy configuration (will be used if criterion_a is not specified)
criterion: crossentropyloss
criterion_params: 
  use_class_weights: False

# --- Classifier-Only Training Configuration (NEW SECTION) ---
classifier_only_training:
  enabled: False               # Set to true to run this phase, false to skip
  num_epochs: 100              # Number of epochs for classifier fine-tuning
  patience: 20                # Early stopping patience for classifier fine-tuning
  use_amp: False             # Can be different from main training, if desired
  clip_grad_norm: 0       # Max norm for gradient clipping for this phase
  train_ratio: 1.0          # Ratio of training set for this phase (usually 1.0)
  first_stage_epochs: 0     # If using a two-stage criterion for classifier training

  optimizer:
    type: Adam # Options: Adam, AdamW, SGD, etc.
    lr: 0.0001    # Learning Rate moved here
    params: # Parameters specific to the optimizer type
      weight_decay: 0.005 # Example for Adam

  scheduler:
    type: StepLR
    # step_size: 5            # Example if using StepLR
    # gamma: 0.5              # Example if using StepLR
    params:                   # Params for ReduceLROnPlateau or other schedulers
      step_size: 7
      gamma: 0.1

  # Optional: Define specific criteria for classifier training
  # If not defined, it might reuse criteria from the main training (depending on main.py logic)
  criterion_a: ghmc
  criterion_a_params:
    # use_class_weights: false # Or true, with weights calculated for this phase if needed
    # gamma: 5.0
    # alpha: 1
  #
  # criterion_b: CrossEntropyLoss # Optional secondary criterion for this phase
  # criterion_b_params:
  #   use_class_weights: false


# --- Results Directory ---
results_dir: 'results_kaggle' # Separate results dir for Kaggle runs

# --- Device Configuration ---
device:
  use_cuda: true      # Attempt to use CUDA (GPU) if available
  multi_gpu: true     # Attempt to use DataParallel if multiple GPUs detected
