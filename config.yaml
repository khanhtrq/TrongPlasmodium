# --- Dataset Configuration ---
data_dir: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/' # Base dir, potentially used by both types
root_dataset_dir: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/cropped_RBCs' # Specific root for annotation images

# --- New Dataset Configuration ---
dataset_config:
  type: 'annotation' # Options: 'annotation', 'imagefolder'
  # --- Settings for 'annotation' type ---
  annotation_train: '/kaggle/input/datn-smote/smote_synthesized_output/synthesized_annotations.txt' # Use SMOTE annotation for training
  annotation_val: 'val_annotation.txt'   # Keep original validation set (relative to data_dir)
  annotation_test: 'test_annotation.txt'  # Keep original test set (relative to data_dir)
  annotation_root: '/kaggle/input/datn-smote/smote_synthesized_output' # Use SMOTE images root for training
  # --- Settings for 'imagefolder' type ---
  imagefolder_root: '/path/to/imagefolder/dataset' # Root directory containing train/val/test subdirs (e.g., /path/to/cifar10_like_data)
  imagefolder_train_subdir: 'train' # Subdirectory for training images (e.g., imagefolder_root/train/class1/img.jpg)
  imagefolder_val_subdir: 'val'   # Subdirectory for validation images
  imagefolder_test_subdir: 'test'  # Subdirectory for test images

batch_size: 32
num_workers: 2 # Adjusted based on typical Kaggle limits

# --- Class Configuration ---
class_names:
  - "Ring"
  - "Trophozoite"
  - "Schizont"
  - "Gametocyte" # Corrected spelling
  - "Healthy RBC"
  - "Other"

# --- Model Configuration ---
model_names:
  # - mobilenetv4_hybrid_large.e600_r384_in1k # Example timm model
  # - resnet50 # Example torchvision model
  - mobilenetv4_hybrid_large.e600_r384_in1k

# --- Training Configuration ---
training:
  num_epochs: 100 # Reduced for example
  patience: 20   # Early stopping patience
  use_amp: false # Use Automatic Mixed Precision (requires CUDA)
  clip_grad_norm: 0 # Max norm for gradient clipping (set to 0 or null to disable)

# --- Optimizer Configuration ---
optimizer:
  type: AdamW # Options: Adam, AdamW, SGD, etc.
  lr: 1e-4    # Learning Rate moved here
  params: # Parameters specific to the optimizer type
    weight_decay: 0.01 # Example for AdamW

# --- LR Scheduler Configuration ---
scheduler:
  type: CosineAnnealingLR # Options: StepLR, ReduceLROnPlateau, CosineAnnealingLR, etc.
  T_max: 100 # For CosineAnnealingLR: Maximum number of iterations (set to num_epochs)
  eta_min: 1e-6 # For CosineAnnealingLR: Minimum learning rate

# --- Loss Function (Criterion) Configuration ---
criterion: CrossEntropyLoss # Options: CrossEntropyLoss, FocalLoss, F1Loss
criterion_params: # Parameters specific to the criterion
  # For FocalLoss:
  # gamma: 2.0
  # reduction: 'mean'
  # For F1Loss:
  # beta: 1.0
  # epsilon: 1e-7
  # For CrossEntropyLoss:
  use_class_weights: false # Set to true to compute and use class weights

# --- Results Directory ---
results_dir: 'results_kaggle' # Separate results dir for Kaggle runs

# --- Device Configuration ---
device:
  use_cuda: true      # Attempt to use CUDA (GPU) if available
  multi_gpu: true     # Attempt to use DataParallel if multiple GPUs detected
