# --- Dataset Configuration ---
data_dir: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/'
root_dataset_dir: '/kaggle/input/datn-cell/Our_Plasmodium_Classification_Khanh_Thesis_Jan2025/cropped_RBCs'
batch_size: 32
num_workers: 2 # Adjusted based on typical Kaggle limits

# --- Class Configuration ---
class_names:
  - "Ring"
  - "Trophozoite"
  - "Schizont"
  - "Gametocyte" # Corrected spelling
  - "Healthy RBC"
  - "Other"

# --- Model Configuration ---
model_names:
  # - mobilenetv4_hybrid_large.e600_r384_in1k # Example timm model
  # - resnet50 # Example torchvision model
  - focalnet_base_lrf # Example timm model

# --- Training Configuration ---
training:
  num_epochs: 100 # Reduced for example
  patience: 20   # Early stopping patience
  use_amp: false # Use Automatic Mixed Precision (requires CUDA)
  clip_grad_norm: 0 # Max norm for gradient clipping (set to 0 or null to disable)

# --- Optimizer Configuration ---
optimizer:
  type: AdamW # Options: Adam, AdamW, SGD, etc.
  lr: 1e-4    # Learning Rate moved here
  params: # Parameters specific to the optimizer type
    weight_decay: 0.01 # Example for AdamW

# --- LR Scheduler Configuration ---
scheduler:
  type: CosineAnnealingLR # Options: StepLR, ReduceLROnPlateau, CosineAnnealingLR, etc.
  T_max: 100 # For CosineAnnealingLR: Maximum number of iterations (set to num_epochs)
  eta_min: 1e-6 # For CosineAnnealingLR: Minimum learning rate

# --- Loss Function (Criterion) Configuration ---
criterion: CrossEntropyLoss # Options: CrossEntropyLoss, FocalLoss, F1Loss
criterion_params: # Parameters specific to the criterion
  # For FocalLoss:
  # gamma: 2.0
  # reduction: 'mean'
  # For F1Loss:
  # beta: 1.0
  # epsilon: 1e-7
  # For CrossEntropyLoss:
  use_class_weights: false # Set to true to compute and use class weights

# --- Results Directory ---
results_dir: 'results_kaggle' # Separate results dir for Kaggle runs

# --- Device Configuration ---
device:
  use_cuda: true      # Attempt to use CUDA (GPU) if available
  multi_gpu: true     # Attempt to use DataParallel if multiple GPUs detected
